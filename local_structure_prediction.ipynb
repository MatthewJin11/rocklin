{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Polypeptide import PPBuilder\n",
    "from Bio.PDB.vectors import calc_dihedral\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, math\n",
    "\n",
    "import urllib.request, urllib.error\n",
    "import nltk\n",
    "\n",
    "def degrees(rad):\n",
    "    return (rad * 180) / math.pi\n",
    "\n",
    "def phi_psi_omega_to_abego(phi, psi, omega):\n",
    "    #if np.isnan(psi): return ‘O’\n",
    "    #if np.isnan(omega): omega = 180\n",
    "    #if np.isnan(phi): phi=90\n",
    "    \n",
    "    if np.isnan(phi) or np.isnan(psi) or np.isnan(omega): \n",
    "        return 'X'\n",
    "    \n",
    "    if abs(omega) < 90:\n",
    "        return 'O'\n",
    "    \n",
    "    elif phi > 0:\n",
    "        if -100.0 <= psi < 100:\n",
    "            return 'G'\n",
    "        else:\n",
    "            return 'E'\n",
    "        \n",
    "    else:\n",
    "        if -75.0 <= psi < 50:\n",
    "            return 'A'\n",
    "        else:\n",
    "            return 'B'\n",
    "        \n",
    "    return 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2018-06-06-ss.cleaned.csv') # read in CSV\n",
    "\n",
    "# first 200 pdb ids\n",
    "input_pdbs = df['pdb_id'][:100].values.T\n",
    "\n",
    "# print(input_pdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "for pdb in input_pdbs:\n",
    "    \n",
    "    try:\n",
    "        if pdb not in os.listdir((os.getcwd() + '/pdb_files/')):\n",
    "            filename = urllib.request.urlretrieve('https://files.rcsb.org/download/{}.pdb'.format(pdb), pdb + '.pdb')\n",
    "            path = os.path.join(os.getcwd(), pdb + '.pdb')\n",
    "            os.rename(path, os.getcwd() + '/pdb_files/' + pdb + '.pdb') \n",
    "            files.append(filename[0])\n",
    "        else:\n",
    "            files.append(pdb)\n",
    "        \n",
    "    except urllib.error.HTTPError:\n",
    "        continue    \n",
    "        \n",
    "files = set(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output set\n",
    "abegopatterns = []\n",
    "\n",
    "# input set\n",
    "seqs = []\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "for file in files:\n",
    "        \n",
    "    phi_psi = []\n",
    "    nres = []\n",
    "    \n",
    "    ran = False\n",
    "    repeat = True\n",
    "        \n",
    "    structure = PDBParser(QUIET = True).get_structure(cwd + \"/pdb_files/\" + file, cwd + \"/pdb_files/\" + file)\n",
    "\n",
    "    for chain in structure:\n",
    "\n",
    "        polypeptides = PPBuilder().build_peptides(chain)\n",
    "\n",
    "        for polypeptide in polypeptides:\n",
    "\n",
    "            ran = True\n",
    "            \n",
    "            # a list of polypeptide chain lengths\n",
    "            nres.append(len(polypeptide))\n",
    "            \n",
    "            if len(nres) > 1:\n",
    "                nres[-1] = nres[-1] + nres[-2]\n",
    "            \n",
    "            phi_psi += polypeptide.get_phi_psi_list()  \n",
    "            \n",
    "            if polypeptide.get_sequence() not in seqs:\n",
    "                repeat = False # don't want duplicate sequences\n",
    "                seqs.append(polypeptide.get_sequence())\n",
    "\n",
    "            break # only the first subunit for now\n",
    "            \n",
    "        break\n",
    "    \n",
    "    if not(ran) or repeat:\n",
    "        continue\n",
    "        \n",
    "    phi_psi_omega = []\n",
    "\n",
    "    residues = [res for res in structure.get_residues()]\n",
    "\n",
    "    for i in range(len(residues) - 1):\n",
    "\n",
    "        if (i + 1) in nres:\n",
    "            omega = None\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                a1 = residues[i]['CA'].get_vector()\n",
    "                a2 = residues[i]['C'].get_vector()\n",
    "                a3 = residues[i + 1]['N'].get_vector()\n",
    "                a4 = residues[i + 1]['CA'].get_vector()\n",
    "                \n",
    "                omega = calc_dihedral(a1,a2,a3,a4)\n",
    "                \n",
    "                phi_psi_omega.append((phi_psi[i][0], phi_psi[i][1], omega))\n",
    "                \n",
    "            except KeyError:\n",
    "                # phi_psi_omega.append((phi_psi[i][0], phi_psi[i][1], None))\n",
    "                # seqs.pop()\n",
    "                continue\n",
    "    \n",
    "    # last triplet tuple\n",
    "    phi_psi_omega.append((phi_psi[-1][0], phi_psi[-1][1], None))\n",
    "    \n",
    "    # ABEGO str\n",
    "    abego = \"\"\n",
    "\n",
    "    for phi, psi, omega in phi_psi_omega: \n",
    "        if phi != None and psi != None and omega != None:\n",
    "            abego += phi_psi_omega_to_abego(degrees(phi), degrees(psi), degrees(omega))\n",
    "        \n",
    "    abegopatterns.append(abego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3564 3564\n"
     ]
    }
   ],
   "source": [
    "print(len(seqs), len(abegopatterns))\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    seqs[i] = str(seqs[i])\n",
    "\n",
    "def seq2ngrams(seqs, n=3):\n",
    "    return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "input_grams = seq2ngrams(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3564, 750), (3564, 750, 6))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# encoder, turns sequence into a fixed vector of numbers\n",
    "# hash function is a possibility\n",
    "tokenizer_encoder = Tokenizer() # Tokenizer Class Instance\n",
    "\n",
    "tokenizer_encoder.fit_on_texts(input_grams) # tokenize the input_grams, updates internal unique vocabulary\n",
    "\n",
    "input_data = tokenizer_encoder.texts_to_sequences(input_grams) # assigns the text a number\n",
    "\n",
    "input_data = pad_sequences(input_data, maxlen=750, padding='post')\n",
    "\n",
    "# decoder\n",
    "tokenizer_decoder = Tokenizer(char_level = True) # every character will be treated as a token because it's ABEGO\n",
    "\n",
    "tokenizer_decoder.fit_on_texts(abegopatterns) \n",
    "\n",
    "target_data = tokenizer_decoder.texts_to_sequences(abegopatterns)\n",
    "\n",
    "target_data = pad_sequences(target_data, maxlen=750, padding='post')\n",
    "\n",
    "target_data = to_categorical(target_data) # oneHotEncoder\n",
    "\n",
    "input_data.shape, target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = np.array([[letter for letter in abego if (letter != None)] for abego in abegopatterns])\n",
    "df = pd.DataFrame(columns=range(len(letters)))\n",
    "for i in range(len(letters)):\n",
    "    df[i] = pd.Series(letters[i])\n",
    "    \n",
    "cat_encoder = OneHotEncoder()\n",
    "cat_encoder.fit(df[[0]])\n",
    "\n",
    "cat_encoder.categories_\n",
    "practice = []\n",
    "for i in range(len(letters)):\n",
    "    practice.append(cat_encoder.transform(df[[i]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "abego_cat_1hot = cat_encoder.fit_transform(df)\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 750)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 750, 750)          6281250   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 750, 128)          417280    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 750, 6)            774       \n",
      "=================================================================\n",
      "Total params: 6,699,304\n",
      "Trainable params: 6,699,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n",
    "\n",
    "maxlen_seq = 750\n",
    "\n",
    "n_words = len(tokenizer_encoder.word_index) + 1 # Number of Possible Amino Acids\n",
    "\n",
    "n_tags = len(tokenizer_decoder.word_index) + 1 # Possible ABEGO Patterns\n",
    "\n",
    "input_seq = Input(shape = (maxlen_seq, )) \n",
    "\n",
    "x = tf.keras.layers.Embedding(input_dim = n_words, output_dim = maxlen_seq, input_length = maxlen_seq)(input_seq)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(LSTM(units = 64, return_sequences = True, recurrent_dropout=0.1))(x)\n",
    "\n",
    "# dense means fully connected\n",
    "output = tf.keras.layers.TimeDistributed(Dense(n_tags, activation = \"softmax\"))(x) \n",
    "\n",
    "model = Model(input_seq, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 750, 750)          2749500   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 750, 200)          680800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 750, 6)            1206      \n",
      "=================================================================\n",
      "Total params: 3,431,506\n",
      "Trainable params: 3,431,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxlen_seq = 750\n",
    "\n",
    "n_words = len(tokenizer_encoder.word_index) + 1 # Number of Possible Amino Acids\n",
    "\n",
    "n_tags = len(tokenizer_decoder.word_index) + 1 # Possible ABEGO Patterns\n",
    "\n",
    "new_model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(maxlen_seq,)),\n",
    "    keras.layers.Embedding(input_dim = n_words, output_dim = maxlen_seq, input_length = maxlen_seq),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(units=100, return_sequences=True)),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(n_tags, activation=\"softmax\"))\n",
    "#   keras.layers.Dense(12000, activation=\"relu\"),\n",
    "#   keras.layers.Embedding(n_words, 750, input_length = 32)\n",
    "#   keras.layers.Dense(1, input_shape=(5,), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "# model.compile defines the loss function, optimizer, and metrics\n",
    "# first metric is Keras provided, second metric is custom metric\n",
    "model.compile(optimizer=\"RMSprop\", loss=\"categorical_crossentropy\", metrics = [\"accuracy\"]) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size = .4, random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_decoder_index = {value:key for key, value in tokenizer_decoder.word_index.items()}\n",
    "reverse_encoder_index = {value:key for key, value in tokenizer_encoder.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806\n",
      "8112\n",
      "1519\n",
      "Sequence: \n",
      " NIIIII\n",
      "Actual: \n",
      " A\n",
      "Predicted: \n",
      " B\n"
     ]
    }
   ],
   "source": [
    "seq = ''\n",
    "pred = ''\n",
    "for amino in X_test[0]:\n",
    "    if amino != 0:\n",
    "        print(amino)\n",
    "        seq += reverse_encoder_index[amino]\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for letter in test_predictions[0]:\n",
    "    if np.argmax(letter) != 0:\n",
    "        pred += reverse_decoder_index[np.argmax(letter)]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "test = ''\n",
    "for letter in y_test[0]:\n",
    "    if np.argmax(letter) != 0:\n",
    "        test += reverse_decoder_index[np.argmax(letter)]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Sequence: \\n\", seq.upper())\n",
    "print(\"Actual: \\n\", test.upper())\n",
    "print(\"Predicted: \\n\", pred.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = new_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bbbbbbbbbbbaaaaaaaaaaaa'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = ''\n",
    "for letter in train_predictions[13]:\n",
    "    if np.argmax(letter) != 0:\n",
    "        train_pred += reverse_decoder_index[np.argmax(letter)]\n",
    "train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for prediction in predictions:\n",
    "#     for letter in prediction:\n",
    "#         if np.argmax(letter) == 5:\n",
    "#             pred += reverse_decoder_index[np.argmax(letter)]\n",
    "len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
